# **Multi-Media Integration Implementation Plan**
## **Complete Implementation Guide for AI Agent**

---

## **üìã Project Overview & Context**

### **Current State**
- **Database**: PostgreSQL with anime-only data (series + characters)
- **Schema**: Already updated with `media_type` and `creator` (JSON) fields
- **Workflow**: pull_from_mal.py ‚Üí character_edit.py ‚Üí hand-edit ‚Üí process_character_final.py ‚Üí upload_to_postgres.py
- **ID System**: Simple sequential IDs for anime only

### **Target State**
- **Multi-media support**: Anime, Visual Novels, Games, Manga, etc.
- **Unified workflow**: Single collection+processing script
- **Couple-based ID management**: `(source_type, source_id)` ‚Üí `unique_id`
- **Continuous ID ranges**: No gaps, sequential assignment
- **Backward compatibility**: Existing anime data preserved
- **Async-first architecture**: All processors use async `pull_raw_data()` for API calls
- **Character naming convention**: Non-anime characters include media type suffix (e.g., "Saber (Visual Novel)")
- **Formal media type display**: Genre field contains formal display names ("Visual Novel", "Manga", etc.)

### **Key Requirements**
1. Modular data collection system
2. Couple-based unique ID management with single continuous range
3. Unified pull+process workflow
4. Complete backward compatibility
5. Migration plan for existing database

### **üîß Key Implementation Updates**

#### **1. Async-First Architecture**
All processors now implement `async def pull_raw_data()` for better API handling:
- **Base Processor**: `async def pull_raw_data()` and `async def pull_and_process()`
- **Anime Processor**: Uses MAL OAuth + async HTTP calls
- **Manga Processor**: Uses MAL OAuth + async Jikan API calls  
- **VNDB Processor**: Uses async VNDB API calls with configuration
- **Game Processors**: Each game uses async patterns for API/scraping

#### **2. Character Naming Convention**
Characters from non-anime sources MUST include media type suffix:
```python
# Correct naming patterns:
"Mikasa Ackerman"           # Anime (no suffix)
"Asuna Yuuki (Manga)"       # Manga version  
"Saber (VN)"                # Visual Novel (short suffix)
"Amber (GI)"                # Game-specific (short suffix)
```

#### **3. Formal Media Type Display**
The `genre` field contains formal display names for UI:
```python
# Genre field values (formal display names):
"Anime"           # for anime characters
"Manga"           # for manga characters  
"Visual Novel"    # for visual novel characters
"Genshin Impact"  # for game characters (use game name)
```

---

## **üóÇÔ∏è Required Context Files**

### **Existing Files to Analyze**
```
CRITICAL - Read these files to understand current implementation:

1. d:\docker\HUST-SE\Geminya\pull_from_mal.py
   - Current anime pulling logic
   - MAL API integration
   - Data format output

2. d:\docker\HUST-SE\Geminya\character_edit.py  
   - Current character processing logic
   - Male filtering, rarity assignment
   - Data cleaning and validation

3. d:\docker\HUST-SE\Geminya\process_character_final.py
   - Final processing before database upload
   - Series-character ID mapping
   - CSV output formatting

4. d:\docker\HUST-SE\Geminya\services\database.py
   - Database schema (series, waifus tables)
   - CRUD operations
   - Current ID handling

5. d:\docker\HUST-SE\Geminya\migrations\001_add_media_type_and_creator.sql
   - Recent schema changes
   - Migration pattern to follow

6. d:\docker\HUST-SE\Geminya\docs\DATABASE_TECHNICAL_DOCUMENTATION.md
   - Database structure documentation
   - Schema definitions
   - Business logic explanations
```

### **Data Files to Examine**
```
IMPORTANT - Check these to understand data structure:

1. data/anime_mal.csv (if exists)
   - Current anime data format
   - Column structure

2. data/characters_mal.csv (if exists)
   - Current character data format  
   - Column structure

3. Any existing CSV files in data/ directory
   - Current data pipeline outputs
   - Format standards to maintain
```

---

## **üèóÔ∏è Implementation Architecture**

### **New File Structure**
```
Root Directory:
‚îú‚îÄ‚îÄ collect_and_process_media.py          # NEW: Main orchestrator
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ input_sources/                    # NEW: Source configurations
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ configs/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ mal_config.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vndb_config.json
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manual_config.json
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ manual_data/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ genshin_series.csv
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ genshin_characters.csv
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ [other manual data files]
‚îÇ   ‚îú‚îÄ‚îÄ registries/                       # NEW: ID management
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ series_id_registry.csv
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ character_id_registry.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ id_manager.py
‚îÇ   ‚îú‚îÄ‚îÄ processors/                       # NEW: Modular processors
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base_processor.py             # Abstract base with async pull_raw_data()
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anime_processor.py            # MAL anime (no config needed)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ manga_processor.py            # MAL manga (no config needed)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vndb_processor.py             # VNDB visual novels (requires config)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ [game]_processor.py           # Each game has dedicated processor
‚îÇ   ‚îú‚îÄ‚îÄ utils/                           # NEW: Helper utilities
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ data_standardizer.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ validator.py
‚îÇ   ‚îú‚îÄ‚îÄ templates/                       # NEW: Manual entry templates
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ series_template.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ characters_template.csv
‚îÇ   ‚îú‚îÄ‚îÄ intermediate/                    # NEW: Processing outputs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ characters_processed.csv
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ series_processed.csv
‚îÇ   ‚îú‚îÄ‚îÄ final/                          # UPDATED: Final outputs
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ characters_cleaned.xlsx      # (hand-edited)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ characters_with_stats.csv    # (LLM generated)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ characters_final.csv         # (database ready)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ series_final.csv             # (database ready)
‚îÇ   ‚îî‚îÄ‚îÄ backups/                        # NEW: Safety backups
‚îÇ       ‚îú‚îÄ‚îÄ [timestamped backup files]
‚îî‚îÄ‚îÄ migrations/                         # EXISTING
    ‚îú‚îÄ‚îÄ 001_add_media_type_and_creator.sql
    ‚îî‚îÄ‚îÄ 002_populate_id_registries.sql   # NEW
```

---

## **üìä ID Management System Design**

### **Core Concept**
```python
# Input: (source_type, source_id) ‚Üí Output: unique_id
# Example mappings:
("mal", "16498") ‚Üí series_id: 1
("mal", "11757") ‚Üí series_id: 2  
("vndb", "v17") ‚Üí series_id: 3
("genshin", "mondstadt") ‚Üí series_id: 4
# Continuous numbering, no gaps
```

### **Registry File Structure**
```csv
# series_id_registry.csv
source_type,source_id,unique_series_id,name,created_at
mal,16498,1,Attack on Titan,2025-01-01 10:00:00
mal,11757,2,Sword Art Online,2025-01-01 10:01:00
vndb,v17,3,Fate/stay night,2025-01-02 09:00:00
genshin,mondstadt,4,Genshin Impact,2025-01-03 11:00:00

# character_id_registry.csv  
source_type,source_id,unique_waifu_id,name,series_unique_id,created_at
mal,40028,1,Mikasa Ackerman,1,2025-01-01 10:00:00
mal,36828,2,Asuna Yuuki,2,2025-01-01 10:01:00
vndb,s83,3,Saber,3,2025-01-02 09:00:00
genshin,amber,4,Amber,4,2025-01-03 11:00:00
```

---

## **üîß Implementation Steps**

### **Phase 1: Infrastructure Setup**

#### **Step 1.1: Create ID Manager System**
```python
# File: data/registries/id_manager.py
class IDManager:
    """Manages couple-based ID assignment with continuous ranges"""
    
    def __init__(self):
        self.series_registry_file = "data/registries/series_id_registry.csv"
        self.character_registry_file = "data/registries/character_id_registry.csv"
        self._ensure_registries_exist()
    
    def get_next_series_id(self) -> int:
        """Get next available series ID (continuous)"""
        # Implementation details below
        
    def get_next_character_id(self) -> int:
        """Get next available character ID (continuous)"""
        # Implementation details below
        
    def get_or_create_series_id(self, source_type: str, source_id: str, name: str) -> int:
        """Get existing ID or create new continuous ID"""
        # Implementation details below
        
    def get_or_create_character_id(self, source_type: str, source_id: str, name: str, series_id: int) -> int:
        """Get existing ID or create new continuous ID"""
        # Implementation details below
        
    def migrate_existing_data(self):
        """Populate registries with existing database data"""
        # Implementation details below

# IMPLEMENTATION REQUIREMENTS:
# - Use pandas for CSV operations
# - Thread-safe file operations (use file locking)  
# - Comprehensive error handling
# - Logging for all ID assignments
# - Backup creation before modifications
```

#### **Step 1.2: Create Base Processor Class**
```python
# File: data/processors/base_processor.py
from abc import ABC, abstractmethod
from typing import List, Dict, Tuple, Optional
import asyncio

class BaseProcessor(ABC):
    """Abstract base class for all media processors"""
    
    def __init__(self, config: Optional[Dict] = None):
        self.config = config or {}
        self.id_manager = IDManager()
        
    @abstractmethod
    def get_source_type(self) -> str:
        """Return the source type identifier (e.g., 'mal', 'vndb')"""
        pass
        
    @abstractmethod
    def is_configured(self) -> bool:
        """Check if processor is properly configured"""
        pass
        
    @abstractmethod
    async def pull_raw_data(self) -> Tuple[List[Dict], List[Dict]]:
        """Pull raw data from source. Returns (series_list, characters_list)"""
        pass
    
    def process_series(self, raw_series: List[Dict]) -> List[Dict]:
        """Process and standardize series data"""
        # Common processing logic
        # Assign unique IDs using id_manager
        # Standardize format
        
    def process_characters(self, raw_characters: List[Dict]) -> List[Dict]:
        """Process and standardize character data"""  
        # Common processing logic (from character_edit.py)
        # Filter males, assign rarities, etc.
        # Add media type suffix to character names (except anime)
        # Set formal genre display names
        
    async def pull_and_process(self) -> Tuple[List[Dict], List[Dict]]:
        """Main entry point: pull and process data"""
        if not self.is_configured():
            return [], []
            
        raw_series, raw_chars = await self.pull_raw_data()  # ASYNC CALL
        processed_series = self.process_series(raw_series)
        processed_chars = self.process_characters(raw_chars)
        
        return processed_chars, processed_series

# IMPLEMENTATION REQUIREMENTS:
# - Extract common logic from existing character_edit.py
# - Implement standardized data format with proper naming
# - Add media type suffixes to non-anime character names
# - Use formal display names in genre field
# - Use async/await pattern for all API calls
# - Use id_manager for all ID assignments
# - Comprehensive logging and error handling
```

#### **Step 1.3: Create Data Standardizer**
```python
# File: data/utils/data_standardizer.py
class DataStandardizer:
    """Standardizes data formats across different sources"""
    
    @staticmethod
    def standardize_series_data(raw_data: Dict, source_type: str) -> Dict:
        """Convert raw series data to standard format"""
        # Standard format:
        # {
        #     'source_type': str,
        #     'source_id': str, 
        #     'name': str,
        #     'english_name': str,
        #     'creator': str (JSON),
        #     'media_type': str,
        #     'image_link': str,
        #     'genres': str,
        #     'synopsis': str,
        #     'favorites': int,
        #     'members': int,
        #     'score': float
        # }
        
    @staticmethod  
    def standardize_character_data(raw_data: Dict, source_type: str) -> Dict:
        """Convert raw character data to standard format"""
        # Standard format:
        # {
        #     'source_type': str,
        #     'source_id': str,
        #     'name': str,               # MUST include media suffix for non-anime
        #     'series': str,
        #     'series_source_id': str,
        #     'genre': str,              # FORMAL display name (e.g., "Visual Novel")
        #     'rarity': int,
        #     'image_url': str,
        #     'about': str,
        #     'favorites': int
        # }
        
        # NAMING CONVENTION REQUIREMENTS:
        # - Anime characters: "Character Name" (no suffix)
        # - Manga characters: "Character Name (Manga)" 
        # - Visual Novel characters: "Character Name (VN)"
        # - Game characters: "Character Name (ShortGameCode)" e.g. "(GI)", "(HSR)"
        #
        # GENRE FIELD REQUIREMENTS (formal display names):
        # - "Anime" for anime characters
        # - "Manga" for manga characters  
        # - "Visual Novel" for VN characters
        # - Specific game names for game characters

# IMPLEMENTATION REQUIREMENTS:
# - Handle missing fields gracefully
# - Validate data types
# - Clean and normalize text fields
# - Convert creator information to JSON format
```

---

## **üéØ Processor Configuration Strategies**

### **Configuration-Free Processors (MAL-based)**
- **Anime Processor**: Uses MAL OAuth credentials from app config, pulls user's anime list
- **Manga Processor**: Uses MAL OAuth credentials from app config, pulls user's manga list
- **Advantage**: No additional setup needed if MAL is configured
- **Data Source**: User's personal MAL lists + Jikan API for details

### **Configuration-Required Processors (API-based)**  
- **VNDB Processor**: Requires `vndb_config.json` with specific VN IDs to process
- **Config Format**:
```json
{
  "vndb_series_ids": ["v17", "v11", "v24", "v4", "v36"],
  "description": "VNDB series IDs to crawl"
}
```
- **Advantage**: Precise control over what data to collect
- **Data Source**: Public VNDB API

### **Manual Data Processors (File-based)**
- **Game Processors**: Each game gets its own processor for manual CSV files
- **Examples**: `genshin_processor.py`, `honkai_processor.py`, `azur_lane_processor.py`
- **Data Source**: Manual CSV files in `data/input_sources/manual_data/`
- **Advantage**: Can handle any data format, no API dependencies

---

### **Phase 2: Processor Implementation**

#### **Step 2.1: Create Anime Processor**
```python
# File: data/processors/anime_processor.py
from .base_processor import BaseProcessor
import json
import asyncio

class AnimeProcessor(BaseProcessor):
    """Processor for MAL anime data"""
    
    def get_source_type(self) -> str:
        return "mal"
        
    def is_configured(self) -> bool:
        """Check if MAL API credentials are available"""
        # Check for MAL API configuration
        # Return True if pull_from_mal.py can run
        
    def pull_raw_data(self) -> Tuple[List[Dict], List[Dict]]:
        """Pull data from MAL using existing pull_from_mal.py logic"""
        # EXTRACT LOGIC FROM: pull_from_mal.py
        # - MAL API authentication
        # - Data fetching loops  
        # - Error handling
        # - Rate limiting
        # Return raw data in original format
        
    def process_series(self, raw_series: List[Dict]) -> List[Dict]:
        """Process anime series data"""
        processed = []
        for series in raw_series:
            # Use standardizer to convert format
            std_data = DataStandardizer.standardize_series_data(series, "mal")
            
            # Assign unique ID
            unique_id = self.id_manager.get_or_create_series_id(
                "mal", 
                std_data['source_id'],
                std_data['name']
            )
            std_data['unique_series_id'] = unique_id
            
            processed.append(std_data)
        return processed
        
    def process_characters(self, raw_characters: List[Dict]) -> List[Dict]:
        """Process anime character data"""
        # EXTRACT LOGIC FROM: character_edit.py
        # - Male filtering logic
        # - Rarity assignment logic  
        # - Data cleaning logic
        # - Apply to each character with ID assignment

# IMPLEMENTATION REQUIREMENTS:
# - Extract ALL logic from pull_from_mal.py
# - Extract ALL logic from character_edit.py (anime parts)
# - Maintain exact same data processing behavior  
# - Add ID management integration
# - Preserve existing error handling
```

#### **Step 2.2: Create Main Orchestrator**  
```python
# File: collect_and_process_media.py
from data.processors.anime_processor import AnimeProcessor
# ... other processors when implemented

class MediaCollectionOrchestrator:
    """Main orchestrator for multi-media data collection"""
    
    def __init__(self):
        self.processors = {
            'anime': AnimeProcessor(),
            # 'visual_novel': VisualNovelProcessor(),  # Future
            # 'game': GameProcessor(),                  # Future
            # 'manga': MangaProcessor(),                # Future
        }
        
    def collect_and_process_all(self):
        """Main entry point for unified data collection"""
        all_characters = []
        all_series = []
        
        # Process each configured media type
        for media_type, processor in self.processors.items():
            if processor.is_configured():
                print(f"Processing {media_type} data...")
                chars, series = processor.pull_and_process()
                all_characters.extend(chars)
                all_series.extend(series)
                print(f"  Added {len(chars)} characters, {len(series)} series")
        
        # Save unified outputs
        self._save_processed_data(all_characters, all_series)
        
        return len(all_characters), len(all_series)
        
    def _save_processed_data(self, characters: List[Dict], series: List[Dict]):
        """Save processed data to intermediate files"""
        # Save to: data/intermediate/characters_processed.csv
        # Save to: data/intermediate/series_processed.csv
        # Create backups of previous files
        
if __name__ == "__main__":
    orchestrator = MediaCollectionOrchestrator()
    char_count, series_count = orchestrator.collect_and_process_all()
    print(f"‚úÖ Processing complete: {char_count} characters, {series_count} series")

# IMPLEMENTATION REQUIREMENTS:
# - Comprehensive error handling
# - Progress reporting
# - Automatic backup creation
# - Validation of outputs
# - Logging of all operations
```

### **Phase 3: Database Migration**

#### **Step 3.1: Create Migration Script**
```sql
-- File: migrations/002_populate_id_registries.sql
-- Populate ID registries with existing database data

-- Create backup tables
CREATE TABLE series_backup_migration AS SELECT * FROM series;
CREATE TABLE waifus_backup_migration AS SELECT * FROM waifus;

-- Validation queries (run these to verify before migration)
SELECT COUNT(*) as series_count FROM series;
SELECT COUNT(*) as character_count FROM waifus;
SELECT MAX(series_id) as max_series_id FROM series;
SELECT MAX(waifu_id) as max_waifu_id FROM waifus;

-- Post-migration validation
-- (These will be populated by the Python migration script)
```

#### **Step 3.2: Create Python Migration Script**
```python
# File: migrate_existing_data.py
from data.registries.id_manager import IDManager
from services.database import DatabaseService

class DatabaseMigration:
    """Migrates existing database data to new ID registry system"""
    
    def __init__(self):
        self.db = DatabaseService()
        self.id_manager = IDManager()
        
    def migrate_all_data(self):
        """Main migration function"""
        print("üöÄ Starting database migration to ID registry system...")
        
        # Step 1: Create registry files
        self._create_registry_files()
        
        # Step 2: Migrate series data
        series_count = self._migrate_series_data()
        
        # Step 3: Migrate character data  
        char_count = self._migrate_character_data()
        
        # Step 4: Validate migration
        self._validate_migration()
        
        print(f"‚úÖ Migration complete: {series_count} series, {char_count} characters")
        
    def _migrate_series_data(self):
        """Migrate existing series to registry"""
        # Get all existing series from database
        # For each series:
        #   - Create registry entry with source_type='mal', source_id=series_id
        #   - Use existing series_id as unique_series_id  
        #   - Preserve all existing data
        
    def _migrate_character_data(self):
        """Migrate existing characters to registry"""  
        # Get all existing characters from database
        # For each character:
        #   - Create registry entry with source_type='mal', source_id=waifu_id
        #   - Use existing waifu_id as unique_waifu_id
        #   - Link to series using series_id
        
    def _validate_migration(self):
        """Validate that migration completed successfully"""
        # Count records in registry vs database
        # Verify all IDs are accounted for
        # Check for any orphaned records

if __name__ == "__main__":
    migration = DatabaseMigration()
    migration.migrate_all_data()

# IMPLEMENTATION REQUIREMENTS:
# - Complete data integrity checks
# - Rollback capability if errors occur
# - Detailed logging of all operations
# - Verification that no data is lost
# - Performance optimization for large datasets
```

### **Phase 4: Update Existing Scripts**

#### **Step 4.1: Update process_character_final.py**
```python
# MODIFICATIONS TO: process_character_final.py

# Change input files:
# OLD: anime_mal.xlsx, characters_with_stats.csv
# NEW: series_processed.csv, characters_with_stats.csv

# Update load_anime_excel method:
def load_series_data(self) -> Optional[pd.DataFrame]:
    """Load the unified series data."""
    try:
        # Change from Excel to CSV
        series_file = "data/intermediate/series_processed.csv" 
        df = pd.read_csv(series_file)
        return df
    except Exception as e:
        # Error handling

# Update processing logic:
# - Handle multiple media types
# - Use unique_series_id from registry
# - Preserve creator JSON format
# - Handle media_type field properly

# IMPLEMENTATION REQUIREMENTS:
# - Maintain backward compatibility
# - Handle mixed media types in single file
# - Preserve all existing validation logic
# - Update column mappings as needed
```

#### **Step 4.2: Create Template System**
```python
# File: data/utils/template_generator.py
class TemplateGenerator:
    """Generates CSV templates for manual data entry"""
    
    def create_series_template(self, media_type: str, output_file: str):
        """Create empty series template CSV"""
        # Create CSV with standard columns
        # Add helpful comments/examples
        
    def create_characters_template(self, series_name: str, media_type: str, output_file: str):
        """Create empty characters template CSV"""
        # Create CSV with standard columns
        # Pre-fill series information
        # Add helpful comments/examples

# IMPLEMENTATION REQUIREMENTS:
# - Clear column headers and examples
# - Validation rules in comments
# - Easy-to-edit format
# - Comprehensive documentation
```

### **Phase 5: Testing & Validation**

#### **Step 5.1: Create Test Suite**
```python
# File: test_multi_media_integration.py
import unittest
from data.registries.id_manager import IDManager
from data.processors.anime_processor import AnimeProcessor

class TestMultiMediaIntegration(unittest.TestCase):
    """Comprehensive test suite for multi-media integration"""
    
    def setUp(self):
        """Set up test environment"""
        # Create test data
        # Initialize test database
        
    def test_id_manager_continuous_assignment(self):
        """Test that IDs are assigned continuously without gaps"""
        
    def test_anime_processor_compatibility(self):
        """Test that anime processor produces same results as old system"""
        
    def test_database_migration_integrity(self):
        """Test that migration preserves all existing data"""
        
    def test_full_workflow_execution(self):
        """Test complete workflow from start to finish"""
        
    def test_mixed_media_processing(self):
        """Test processing multiple media types together"""

# IMPLEMENTATION REQUIREMENTS:
# - Test with real data samples
# - Validate data integrity at each step
# - Performance testing with large datasets
# - Error condition testing
# - Rollback testing
```

---

## **üìã Implementation Checklist**

### **Phase 1: Infrastructure (Week 1)**
- [ ] **Create `data/registries/id_manager.py`**
  - [ ] Implement continuous ID assignment logic
  - [ ] Add file locking for thread safety
  - [ ] Create registry CSV structure
  - [ ] Add migration methods for existing data
  - [ ] Comprehensive error handling and logging

- [ ] **Create `data/processors/base_processor.py`**
  - [ ] Define abstract interface for all processors
  - [ ] Extract common processing logic from character_edit.py
  - [ ] Implement standardized data format conversion
  - [ ] Add ID management integration
  - [ ] Create validation framework

- [ ] **Create `data/utils/data_standardizer.py`**
  - [ ] Implement series data standardization
  - [ ] Implement character data standardization  
  - [ ] Handle creator field JSON conversion
  - [ ] Add data validation and cleaning
  - [ ] Support for multiple source formats

### **Phase 2: Anime Migration (Week 2)**
- [ ] **Create `data/processors/anime_processor.py`**
  - [ ] Extract all logic from pull_from_mal.py
  - [ ] Extract all logic from character_edit.py  
  - [ ] Maintain exact same data processing behavior
  - [ ] Integrate with ID manager
  - [ ] Add comprehensive error handling

- [ ] **Create `collect_and_process_media.py`**
  - [ ] Implement main orchestrator class
  - [ ] Add support for anime processor
  - [ ] Create unified output generation
  - [ ] Add progress reporting and logging
  - [ ] Implement backup creation

- [ ] **Test with existing anime data**
  - [ ] Verify identical output to current system
  - [ ] Validate ID assignment works correctly
  - [ ] Test error handling and edge cases
  - [ ] Performance testing with full dataset

### **Phase 3: Database Migration (Week 3)**
- [ ] **Create `migrations/002_populate_id_registries.sql`**
  - [ ] Add validation queries
  - [ ] Create backup table commands
  - [ ] Add verification steps

- [ ] **Create `migrate_existing_data.py`**
  - [ ] Implement series data migration
  - [ ] Implement character data migration
  - [ ] Add comprehensive validation
  - [ ] Create rollback capability
  - [ ] Add detailed logging

- [ ] **Execute migration on test database**
  - [ ] Run full migration process
  - [ ] Validate all data preserved
  - [ ] Test rollback procedures
  - [ ] Performance optimization if needed

### **Phase 4: Integration Testing (Week 4)**
- [ ] **Update process_character_final.py**
  - [ ] Change input files to use processed data
  - [ ] Handle multiple media types
  - [ ] Update column mappings
  - [ ] Preserve all existing validation

- [ ] **Create template system**
  - [ ] Implement template generator
  - [ ] Create documentation for manual data entry
  - [ ] Test template validation

- [ ] **Full system testing**
  - [ ] Test complete workflow end-to-end
  - [ ] Validate database uploads work correctly
  - [ ] Test UI displays with new data format
  - [ ] Performance testing with mixed media types

### **Phase 5: Future Expansion (Week 5+)**
- [ ] **Create additional processors**
  - [ ] Visual novel processor (VNDB integration)
  - [ ] Game processor (manual/wiki scraping)
  - [ ] Manga processor (MangaDex integration)
  - [ ] Test each processor individually

- [ ] **Documentation and training**
  - [ ] Update user documentation
  - [ ] Create troubleshooting guides
  - [ ] Document manual data entry process
  - [ ] Create runbooks for common operations

---

## **‚ö†Ô∏è Critical Success Factors**

### **Data Integrity**
- **NEVER** modify existing IDs in the database
- **ALWAYS** create backups before any migration step
- **VALIDATE** every step with counts and checksums
- **TEST** rollback procedures before production migration

### **Backward Compatibility** 
- **PRESERVE** exact same anime processing behavior
- **MAINTAIN** existing file format compatibility where possible
- **ENSURE** existing UI code works without changes
- **VALIDATE** existing workflows continue to function

### **Performance Requirements**
- **OPTIMIZE** for datasets up to 50,000 characters
- **IMPLEMENT** progress reporting for long operations
- **USE** efficient file I/O and database operations
- **MONITOR** memory usage during processing

### **Error Handling**
- **LOG** all operations with timestamps and details
- **HANDLE** API rate limits and network errors gracefully
- **PROVIDE** clear error messages and recovery suggestions
- **IMPLEMENT** automatic retry logic where appropriate

---

## **üöÄ Execution Commands**

### **Initial Setup**
```bash
# Create directory structure
mkdir -p data/{registries,processors,utils,templates,intermediate,final,backups}
mkdir -p data/input_sources/{configs,manual_data}

# Run migration (TEST ENVIRONMENT FIRST!)
python migrate_existing_data.py

# Test new system with anime data only  
python collect_and_process_media.py
```

### **Production Workflow**
```bash
# Step 1: Unified data collection and processing
python collect_and_process_media.py
# ‚Üí Outputs: data/intermediate/characters_processed.csv, series_processed.csv

# Step 2: Hand-edit (unchanged process)
# Edit characters_processed.csv ‚Üí save as data/final/characters_cleaned.xlsx
# Edit series_processed.csv ‚Üí save as data/final/series_cleaned.xlsx

# Step 3: LLM stats generation (existing script, updated paths)
python generate_stats_with_llm.py  
# ‚Üí data/final/characters_with_stats.csv

# Step 4: Final processing (updated script)
python process_character_final.py
# ‚Üí data/final/characters_final.csv, series_final.csv

# Step 5: Database upload (existing script)
python upload_to_postgres.py
```

---

## **üìã Implementation Summary**

### **‚úÖ Completed Changes**
1. **Async Architecture**: Updated all processors to use `async def pull_raw_data()` pattern
2. **Character Naming**: Non-anime characters use short suffixes - "(Manga)", "(VN)", "(GI)", etc.
3. **Formal Genre Display**: Genre field contains formal names ("Manga", "Visual Novel", etc.)

### **üéØ Key Architectural Decisions**
- **MAL Processors**: No configuration needed (anime, manga) - uses user's MAL lists
- **VNDB Processor**: Requires configuration file with specific VN IDs to process
- **Game Processors**: Each game gets dedicated processor for maximum flexibility
- **Manual Data**: Supports CSV file processing for any custom data sources

### **üîÑ Migration Path**
1. All existing anime data preserved with backward compatibility
2. New processors add multi-media support without breaking existing workflows
3. Character naming conventions ensure no conflicts between media types
4. Database schema supports new media types through existing `media_type` field

This comprehensive plan provides everything needed to implement the multi-media integration system while maintaining full backward compatibility and data integrity.